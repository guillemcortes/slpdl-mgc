{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feature_extraction as fe\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing:  100\n",
      "Currently processing:  200\n",
      "Currently processing:  300\n",
      "Currently processing:  400\n",
      "Currently processing:  500\n",
      "Currently processing:  600\n",
      "Currently processing:  700\n",
      "Currently processing:  800\n",
      "Currently processing:  900\n",
      "Currently processing:  1000\n"
     ]
    }
   ],
   "source": [
    "spectrograms, labels = fe.extract_spectrograms('../datasets/GTZAN.gui')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trainval, x_test, y_trainval, y_test = train_test_split(spectrograms, labels, test_size=0.2)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_trainval, y_trainval, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 training samples\n",
      "200 validation samples\n",
      "200 test samples\n"
     ]
    }
   ],
   "source": [
    "train_data = [(x, y) for x, y in zip(x_train, y_train)]\n",
    "val_data = [(x, y) for x, y in zip(x_val, y_val)]\n",
    "test_data = [(x, y) for x, y in zip(x_test, y_test)]\n",
    "\n",
    "print(len(train_data), \"training samples\")\n",
    "print(len(val_data), \"validation samples\")\n",
    "print(len(test_data), \"test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1111\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Hyper-parameters \n",
    "input_size = 25\n",
    "hidden_size = [256, 128, 64]\n",
    "#hidden_size = [25, 20, 15]\n",
    "kernel_size = 5\n",
    "\n",
    "num_classes = 10\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "n_features = 1 #Get from xtrain (see BatchNorm1d)\n",
    "\n",
    "# Fully connected neural network with one hidden layer\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.conv1d1 = nn.Conv1d(input_size, hidden_size[0], kernel_size)\n",
    "        self.conv1d1_bn = nn.BatchNorm1d(n_features, momentum=0.9)\n",
    "        self.pool1 = torch.nn.MaxPool1d(kernel_size=2)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.conv1d2 = nn.Conv1d(input_size, hidden_size[1], kernel_size)\n",
    "        self.conv1d2_bn = nn.BatchNorm1d(n_features, momentum=0.9)\n",
    "        self.pool2 = torch.nn.MaxPool1d(kernel_size=2)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.conv1d3 = nn.Conv1d(input_size, hidden_size[2], kernel_size)\n",
    "        self.conv1d3_bn = nn.BatchNorm1d(n_features, momentum=0.9)\n",
    "        self.pool3 = torch.nn.MaxPool1d(kernel_size=2)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size[0], num_classes)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        #order: conv, batchnorm, activation, maxpool, drop\n",
    "        #Calls:\n",
    "        #Convolution 1d: out = F.relu(self.conv1d1(x))\n",
    "        #Batch           normalization: out = self.conv1d1_bn(out)\n",
    "        #Dropout:        out = self.dropout1(out)\n",
    "        #Max Pooling:    out = self.pool1(out)\n",
    "        \n",
    "        #Layer1\n",
    "        out = self.relu(self.conv1d1_bn(self.conv1d1(x)))\n",
    "        out = self.pool1(out)\n",
    "        out = self.dropout1(out)\n",
    "        \n",
    "        #Layer2\n",
    "        out = self.relu(self.conv1d2_bn(self.conv1d2(x)))\n",
    "        out = self.pool2(out)\n",
    "        out = self.dropout2(out)\n",
    "        \n",
    "        #Layer3\n",
    "        out = self.relu(self.conv1d2_bn(self.conv1d2(x)))\n",
    "        out = self.pool2(out)\n",
    "        out = self.dropout2(out)\n",
    "        \n",
    "        #TimeDistributed\n",
    "        \n",
    "        \n",
    "        #Time_distributed_merge_layer\n",
    "        \n",
    "        \n",
    "        #Softmax\n",
    "        out = self.softmax(self.fc(out))\n",
    "        \n",
    "        return out # we can return here the softmax, but then we should use the nll_loss instead of the cross_entropy\n",
    "    \n",
    "\n",
    "model = CNNClassifier(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
