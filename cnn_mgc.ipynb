{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feature_extraction as fe\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "seed = 3\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def data_batcher(data, batch_size):\n",
    "    random.shuffle(data)\n",
    "    return [data[i:i + batch_size] for i in range(0, len(data), batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 features extracted\n"
     ]
    }
   ],
   "source": [
    "dataset = 'GTZAN'\n",
    "new_feats = False\n",
    "\n",
    "\n",
    "dataset_filename = f'../datasets/{dataset}.gui'\n",
    "pickle_filename = f'./feats_spects-pickles/{dataset_filename.split(\"/\")[-1][:-4]}_feats.pkl'\n",
    "if new_feats or not os.path.isfile(pickle_filename):\n",
    "    spectrograms, labels = fe.extract_spectrograms(dataset_filename)\n",
    "    with open(pickle_filename, 'wb') as f:\n",
    "        pickle.dump([feats, labels], f)\n",
    "else:\n",
    "    with open(pickle_filename, 'rb') as f:\n",
    "        feats, labels = pickle.load(f)\n",
    "    print(f'{feats.shape[0]} features extracted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 training samples\n",
      "200 validation samples\n",
      "200 test samples\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_feats = scaler.fit_transform(feats)\n",
    "\n",
    "x_trainval, x_test, y_trainval, y_test = train_test_split(scaled_feats, labels, test_size=0.2)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_trainval, y_trainval, test_size=0.25)\n",
    "\n",
    "train_data = [(x, y) for x, y in zip(x_train, y_train)]\n",
    "val_data = [(x, y) for x, y in zip(x_val, y_val)]\n",
    "test_data = [(x, y) for x, y in zip(x_test, y_test)]\n",
    "\n",
    "print(len(train_data), \"training samples\")\n",
    "print(len(val_data), \"validation samples\")\n",
    "print(len(test_data), \"test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "input_size = 25\n",
    "hidden_size = [256, 128, 64]\n",
    "n_features = [256, 128, 64] #Get from xtrain (see BatchNorm1d)\n",
    "kernel_size = 5\n",
    "\n",
    "num_classes = 10\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "input_size = 25\n",
    "hidden_size = [256, 128, 64, 32]\n",
    "num_classes = 10\n",
    "n_features = [256, 128, 64] #Get from xtrain (see BatchNorm1d)\n",
    "kernel_size = 5\n",
    "train_batch_size = 100\n",
    "valid_batch_size = 100\n",
    "\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.conv1d1 = nn.Conv1d(input_size, hidden_size[0], kernel_size)\n",
    "        self.conv1d1_bn = nn.BatchNorm1d(n_features, momentum=0.9)\n",
    "        self.pool1 = torch.nn.MaxPool1d(kernel_size=2)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.conv1d2 = nn.Conv1d(input_size, hidden_size[1], kernel_size)\n",
    "        self.conv1d2_bn = nn.BatchNorm1d(n_features, momentum=0.9)\n",
    "        self.pool2 = torch.nn.MaxPool1d(kernel_size=2)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.conv1d3 = nn.Conv1d(input_size, hidden_size[2], kernel_size)\n",
    "        self.conv1d3_bn = nn.BatchNorm1d(n_features, momentum=0.9)\n",
    "        self.pool3 = torch.nn.MaxPool1d(kernel_size=2)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size[0], num_classes)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        #order: conv, batchnorm, activation, maxpool, drop\n",
    "        #Calls:\n",
    "        #Convolution 1d: out = F.relu(self.conv1d1(x))\n",
    "        #Batch           normalization: out = self.conv1d1_bn(out)\n",
    "        #Dropout:        out = self.dropout1(out)\n",
    "        #Max Pooling:    out = self.pool1(out)\n",
    "        \n",
    "        #Layer1\n",
    "        out = F.relu(self.conv1d1_bn(self.conv1d1(x)))\n",
    "        out = self.pool1(out)\n",
    "        out = self.dropout1(out)\n",
    "        \n",
    "        #Layer2\n",
    "        out = F.relu(self.conv1d2_bn(self.conv1d2(x)))\n",
    "        out = self.pool2(out)\n",
    "        out = self.dropout2(out)\n",
    "        \n",
    "        #Layer3\n",
    "        out = F.relu(self.conv1d2_bn(self.conv1d2(x)))\n",
    "        out = self.pool2(out)\n",
    "        out = self.dropout2(out)\n",
    "        \n",
    "        #TimeDistributed\n",
    "        \n",
    "        \n",
    "        #Time_distributed_merge_layer\n",
    "        \n",
    "        \n",
    "        #Softmax\n",
    "        out = self.softmax(self.fc(out))\n",
    "        \n",
    "        return out # we can return here the softmax, but then we should use the nll_loss instead of the cross_entropy\n",
    "    \n",
    "\n",
    "model = CNNClassifier(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(log_interval, model, device, data_batches, optimizer, epoch):\n",
    "    losses = []\n",
    "    len_glob_data = np.prod(np.array(np.array(data_batches).shape[:-1]))\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i, batch in enumerate(data_batches):\n",
    "        x, y = list(zip(*batch))\n",
    "        \n",
    "        # Move tensors to the configured device\n",
    "        data = torch.from_numpy(np.array(x)).float().to(device)\n",
    "        target = torch.from_numpy(np.array(y)).to(device)\n",
    "        \n",
    "        # clear all the gradients of the optimized tensors\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if (i+1) % log_interval == 0:\n",
    "            print(f'Train Epoch: {epoch+1} [{(i+1)*len(data)}/{len_glob_data} ({str(int(100. * (i+1)*len(data) / len_glob_data)).zfill(2)}%)]\\tLoss: {round(loss.item(),6)}')\n",
    "    return np.array(losses).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, device, data_batches):\n",
    "    len_glob_data = np.prod(np.array(np.array(data_batches).shape[:-1]))\n",
    "    \n",
    "    model.eval()  # let's put the model in evaluation mode\n",
    "\n",
    "    validation_loss = []\n",
    "    correct = 0\n",
    "    with torch.no_grad():  # we don't need gradient computation at all\n",
    "        for i, batch in enumerate(data_batches):\n",
    "            x, y = list(zip(*batch))\n",
    "        \n",
    "            data = torch.from_numpy(np.array(x)).float().to(device)\n",
    "            target = torch.from_numpy(np.array(y)).to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            validation_loss.append(loss.item())\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    validation_loss = np.array(validation_loss).mean()\n",
    "    accuracy = 100. * correct / len_glob_data\n",
    "\n",
    "    print(f'\\nValidation set: Average loss: {round(validation_loss, 4)}, Accuracy: {correct}/{len_glob_data} ({int(accuracy)}%)\\n')\n",
    "    \n",
    "    return accuracy, validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "input_size = 25\n",
    "hidden_size = [256, 128, 64, 32]\n",
    "num_classes = 10\n",
    "\n",
    "train_batch_size = 100\n",
    "valid_batch_size = 100\n",
    "\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3-dimensional input for 3-dimensional weight 256 25, but got 2-dimensional input of size [100, 25] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-34dc200dff3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-25ddc417eae3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(log_interval, model, device, data_batches, optimizer, epoch)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/slpdl-project/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-54d9a4abea28>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m#Layer1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1d1_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1d1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/slpdl-project/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/slpdl-project/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    194\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m    195\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 196\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 3-dimensional input for 3-dimensional weight 256 25, but got 2-dimensional input of size [100, 25] instead"
     ]
    }
   ],
   "source": [
    "train_data_batches = data_batcher(train_data, train_batch_size)\n",
    "val_data_batches = data_batcher(val_data, valid_batch_size)\n",
    "\n",
    "model = CNNClassifier(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_loss = train(2, model, device, train_data_batches, optimizer, epoch)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    accuracy, valid_loss = validate(model, device, val_data_batches)\n",
    "    valid_losses.append(valid_loss)\n",
    "    accuracies.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.subplots_adjust(right=2.5)\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.set_title('Loss')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.plot(train_losses, 'b-', label='Train')\n",
    "ax1.plot(valid_losses, 'r-', label='Validation')\n",
    "ax1.legend(loc='upper right')\n",
    "\n",
    "ax1 = fig.add_subplot(122)\n",
    "ax1.set_title('Accuracy')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Accuracy (%)')\n",
    "ax1.plot(accuracies, 'b-')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
